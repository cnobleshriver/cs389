{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPSCI 389: Introduction to Machine Learning\n",
    "# Model Evaluation\n",
    "\n",
    "In this notebook we will consider ways of evaluating how effective supervised learning algorithms are.\n",
    "\n",
    "Let's start with the imports that we will use in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import KDTree\n",
    "from sklearn.base import BaseEstimator\n",
    "import numpy as np\n",
    "\n",
    "# New this time:\n",
    "from sklearn.model_selection import train_test_split    # For splitting into training and testing sets (more on this below!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's load and display the GPA data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>physics</th>\n",
       "      <th>biology</th>\n",
       "      <th>history</th>\n",
       "      <th>English</th>\n",
       "      <th>geography</th>\n",
       "      <th>literature</th>\n",
       "      <th>Portuguese</th>\n",
       "      <th>math</th>\n",
       "      <th>chemistry</th>\n",
       "      <th>gpa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>622.60</td>\n",
       "      <td>491.56</td>\n",
       "      <td>439.93</td>\n",
       "      <td>707.64</td>\n",
       "      <td>663.65</td>\n",
       "      <td>557.09</td>\n",
       "      <td>711.37</td>\n",
       "      <td>731.31</td>\n",
       "      <td>509.80</td>\n",
       "      <td>1.33333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>538.00</td>\n",
       "      <td>490.58</td>\n",
       "      <td>406.59</td>\n",
       "      <td>529.05</td>\n",
       "      <td>532.28</td>\n",
       "      <td>447.23</td>\n",
       "      <td>527.58</td>\n",
       "      <td>379.14</td>\n",
       "      <td>488.64</td>\n",
       "      <td>2.98333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>455.18</td>\n",
       "      <td>440.00</td>\n",
       "      <td>570.86</td>\n",
       "      <td>417.54</td>\n",
       "      <td>453.53</td>\n",
       "      <td>425.87</td>\n",
       "      <td>475.63</td>\n",
       "      <td>476.11</td>\n",
       "      <td>407.15</td>\n",
       "      <td>1.97333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>756.91</td>\n",
       "      <td>679.62</td>\n",
       "      <td>531.28</td>\n",
       "      <td>583.63</td>\n",
       "      <td>534.42</td>\n",
       "      <td>521.40</td>\n",
       "      <td>592.41</td>\n",
       "      <td>783.76</td>\n",
       "      <td>588.26</td>\n",
       "      <td>2.53333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>584.54</td>\n",
       "      <td>649.84</td>\n",
       "      <td>637.43</td>\n",
       "      <td>609.06</td>\n",
       "      <td>670.46</td>\n",
       "      <td>515.38</td>\n",
       "      <td>572.52</td>\n",
       "      <td>581.25</td>\n",
       "      <td>529.04</td>\n",
       "      <td>1.58667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43298</th>\n",
       "      <td>519.55</td>\n",
       "      <td>622.20</td>\n",
       "      <td>660.90</td>\n",
       "      <td>543.48</td>\n",
       "      <td>643.05</td>\n",
       "      <td>579.90</td>\n",
       "      <td>584.80</td>\n",
       "      <td>581.25</td>\n",
       "      <td>573.92</td>\n",
       "      <td>2.76333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43299</th>\n",
       "      <td>816.39</td>\n",
       "      <td>851.95</td>\n",
       "      <td>732.39</td>\n",
       "      <td>621.63</td>\n",
       "      <td>810.68</td>\n",
       "      <td>666.79</td>\n",
       "      <td>705.22</td>\n",
       "      <td>781.01</td>\n",
       "      <td>831.76</td>\n",
       "      <td>3.81667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43300</th>\n",
       "      <td>798.75</td>\n",
       "      <td>817.58</td>\n",
       "      <td>731.98</td>\n",
       "      <td>648.42</td>\n",
       "      <td>751.30</td>\n",
       "      <td>648.67</td>\n",
       "      <td>662.05</td>\n",
       "      <td>773.15</td>\n",
       "      <td>835.25</td>\n",
       "      <td>3.75000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43301</th>\n",
       "      <td>527.66</td>\n",
       "      <td>443.82</td>\n",
       "      <td>545.88</td>\n",
       "      <td>624.18</td>\n",
       "      <td>420.25</td>\n",
       "      <td>676.80</td>\n",
       "      <td>583.41</td>\n",
       "      <td>395.46</td>\n",
       "      <td>509.80</td>\n",
       "      <td>2.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43302</th>\n",
       "      <td>512.56</td>\n",
       "      <td>415.41</td>\n",
       "      <td>517.36</td>\n",
       "      <td>532.37</td>\n",
       "      <td>592.30</td>\n",
       "      <td>382.20</td>\n",
       "      <td>538.35</td>\n",
       "      <td>448.02</td>\n",
       "      <td>496.39</td>\n",
       "      <td>3.16667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43303 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       physics  biology  history  English  geography  literature  Portuguese  \\\n",
       "0       622.60   491.56   439.93   707.64     663.65      557.09      711.37   \n",
       "1       538.00   490.58   406.59   529.05     532.28      447.23      527.58   \n",
       "2       455.18   440.00   570.86   417.54     453.53      425.87      475.63   \n",
       "3       756.91   679.62   531.28   583.63     534.42      521.40      592.41   \n",
       "4       584.54   649.84   637.43   609.06     670.46      515.38      572.52   \n",
       "...        ...      ...      ...      ...        ...         ...         ...   \n",
       "43298   519.55   622.20   660.90   543.48     643.05      579.90      584.80   \n",
       "43299   816.39   851.95   732.39   621.63     810.68      666.79      705.22   \n",
       "43300   798.75   817.58   731.98   648.42     751.30      648.67      662.05   \n",
       "43301   527.66   443.82   545.88   624.18     420.25      676.80      583.41   \n",
       "43302   512.56   415.41   517.36   532.37     592.30      382.20      538.35   \n",
       "\n",
       "         math  chemistry      gpa  \n",
       "0      731.31     509.80  1.33333  \n",
       "1      379.14     488.64  2.98333  \n",
       "2      476.11     407.15  1.97333  \n",
       "3      783.76     588.26  2.53333  \n",
       "4      581.25     529.04  1.58667  \n",
       "...       ...        ...      ...  \n",
       "43298  581.25     573.92  2.76333  \n",
       "43299  781.01     831.76  3.81667  \n",
       "43300  773.15     835.25  3.75000  \n",
       "43301  395.46     509.80  2.50000  \n",
       "43302  448.02     496.39  3.16667  \n",
       "\n",
       "[43303 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the data set\n",
    "df = pd.read_csv(\"https://people.cs.umass.edu/~pthomas/courses/COMPSCI_389_Spring2024/GPA.csv\", delimiter=',') # Read GPA.csv, assuming numbers are separated by commas\n",
    "#df = pd.read_csv(\"data/GPA.csv\", delimiter=',')\n",
    "\n",
    "# Display the data set\n",
    "display(df)\n",
    "\n",
    "# Split into X (inputs) and y (labels)\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall our nearest neighbor implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NearestNeighbor(BaseEstimator):\n",
    "    def fit(self, X, y):\n",
    "        # Convert X and y to NumPy arrays if they are DataFrames. \n",
    "        # This makes fit compatible with numpy arrays or DataFrames\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.values\n",
    "\n",
    "        # Store the training data and labels.\n",
    "        self.X_data = X\n",
    "        self.y_data = y\n",
    "        \n",
    "        # Create a KDTree for efficient nearest neighbor search\n",
    "        self.tree = KDTree(X)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Convert X to a NumPy array if it's a DataFrame\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "\n",
    "        # Query the tree for the nearest neighbors of all points in X.\n",
    "        # ind will be a 2D array where ind[i,j] is the index of the \n",
    "        # j'th nearest point to the i'th row in X.\n",
    "        dist, ind = self.tree.query(X, k=1)\n",
    "\n",
    "        # Extract the nearest labels.\n",
    "        # ind[:,0] are the indices of the nearest neighbors to each \n",
    "        # query (each row in x))\n",
    "        return self.y_data[ind[:,0]]        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Now that we have created our first ML algorithm, how we can we determine how effective it is?\n",
    "\n",
    "> **Idea**: Run the model on many data points and compute the average error.\n",
    "\n",
    "Let's do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Error: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Train the model on the data\n",
    "model = NearestNeighbor()\n",
    "model.fit(X, y)\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Compute the average error\n",
    "average_error = (predictions - y).mean()\n",
    "\n",
    "print(\"Average Error:\", average_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Illusion of Perfect Predictions\n",
    "\n",
    "We've seemingly achieved perfect predictions with our model! But let's pause and reflect.\n",
    "\n",
    "**Question**: Are our predictions genuinely perfect?\n",
    "\n",
    "**Answer**: Not quite. There's a fundamental problem with our approach: we evaluated our model's performance using the **same data** we used to train it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why This Evaluation Is Misleading\n",
    "\n",
    "Evaluating a model on the training data answers the question:\n",
    "\n",
    "> How well does our model predict outcomes for data it has already seen?\n",
    "\n",
    "But the real question we want to answer is:\n",
    "\n",
    "> How well can our model predict outcomes for new, unseen data?\n",
    "\n",
    "This is not only a problem for the NN algorithm (although it is particularly clear in this case). This problem arises when you evaluate *any* ML algorithm using the same data (or some of the same data) that was used to train it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Test Splits\n",
    "\n",
    "To accurately assess a model's performance, we need to test it on data that it hasn't seen during training. This is where **train/test splits** come into play.\n",
    "\n",
    "- **Training Set**: A subset of the data used to train the model. The model learns to make predictions based on this data.\n",
    "- **Testing Set**: A separate subset used to evaluate the model. This set is not used during training and thus provides an unbiased evaluation of the model's performance on new data.\n",
    "\n",
    "By splitting our data into these two sets, we can train our model on one portion and then test its predictions on another, unseen portion. This approach gives us a more realistic measure of how well our model will perform in real-world scenarios, where it encounters data it hasn't seen before.\n",
    "\n",
    "This raises the question: If we have `data_size` points (rows), how many should we use for training and how many for testing?\n",
    "\n",
    "- If we use too much for training, our evaluation will have high variance (it will not be reliable).\n",
    "- If we use too little for training, the models we learn will not perform well.\n",
    "\n",
    "Although there is some research studying how to split data into training and testing sets, the *vast* majority of the time people pick a split like 50/50, 60/40, 40/60, 80/20, 20/80, etc. based on their intuition about how much data their algorithm needs to produce a good model and how much data will be needed for evaluation. Let's use 80% of our data for training and 20% for testing.\n",
    "\n",
    "**Question**: If we take the first X% for training and the last (100-X)% for testing, what's something we should watch out for in real applications?\n",
    "\n",
    "**Answer**: Sometimes data sets are provided in some sort of order. For example, the student data could be sorted by GPA. We don't want to put all of the high-GPA points into training and the low-GPA points into testing, since that would also bias our evaluation. We therefore randomly select which points go into the training and testing sets. We will use the `train_test_split` function from scikit-learn which does this for us (when `shuffle=True`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>physics</th>\n",
       "      <th>biology</th>\n",
       "      <th>history</th>\n",
       "      <th>English</th>\n",
       "      <th>geography</th>\n",
       "      <th>literature</th>\n",
       "      <th>Portuguese</th>\n",
       "      <th>math</th>\n",
       "      <th>chemistry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28005</th>\n",
       "      <td>405.95</td>\n",
       "      <td>441.07</td>\n",
       "      <td>543.93</td>\n",
       "      <td>642.68</td>\n",
       "      <td>450.98</td>\n",
       "      <td>525.70</td>\n",
       "      <td>561.74</td>\n",
       "      <td>429.42</td>\n",
       "      <td>447.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17302</th>\n",
       "      <td>509.22</td>\n",
       "      <td>514.19</td>\n",
       "      <td>476.64</td>\n",
       "      <td>518.26</td>\n",
       "      <td>470.93</td>\n",
       "      <td>560.50</td>\n",
       "      <td>449.40</td>\n",
       "      <td>393.80</td>\n",
       "      <td>410.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27044</th>\n",
       "      <td>505.69</td>\n",
       "      <td>586.47</td>\n",
       "      <td>676.95</td>\n",
       "      <td>740.67</td>\n",
       "      <td>557.43</td>\n",
       "      <td>573.57</td>\n",
       "      <td>684.98</td>\n",
       "      <td>626.14</td>\n",
       "      <td>558.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31774</th>\n",
       "      <td>432.28</td>\n",
       "      <td>621.59</td>\n",
       "      <td>515.54</td>\n",
       "      <td>469.30</td>\n",
       "      <td>537.18</td>\n",
       "      <td>549.01</td>\n",
       "      <td>426.03</td>\n",
       "      <td>520.14</td>\n",
       "      <td>566.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1483</th>\n",
       "      <td>670.08</td>\n",
       "      <td>515.43</td>\n",
       "      <td>598.85</td>\n",
       "      <td>686.78</td>\n",
       "      <td>636.61</td>\n",
       "      <td>652.86</td>\n",
       "      <td>508.80</td>\n",
       "      <td>635.35</td>\n",
       "      <td>819.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18630</th>\n",
       "      <td>447.26</td>\n",
       "      <td>419.96</td>\n",
       "      <td>514.69</td>\n",
       "      <td>360.20</td>\n",
       "      <td>576.42</td>\n",
       "      <td>435.27</td>\n",
       "      <td>458.60</td>\n",
       "      <td>422.37</td>\n",
       "      <td>437.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23873</th>\n",
       "      <td>679.87</td>\n",
       "      <td>713.81</td>\n",
       "      <td>679.53</td>\n",
       "      <td>708.97</td>\n",
       "      <td>678.52</td>\n",
       "      <td>739.79</td>\n",
       "      <td>724.57</td>\n",
       "      <td>653.90</td>\n",
       "      <td>767.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10573</th>\n",
       "      <td>584.54</td>\n",
       "      <td>677.47</td>\n",
       "      <td>613.97</td>\n",
       "      <td>543.48</td>\n",
       "      <td>560.80</td>\n",
       "      <td>579.90</td>\n",
       "      <td>510.76</td>\n",
       "      <td>489.41</td>\n",
       "      <td>484.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10922</th>\n",
       "      <td>607.08</td>\n",
       "      <td>611.81</td>\n",
       "      <td>491.59</td>\n",
       "      <td>577.75</td>\n",
       "      <td>557.43</td>\n",
       "      <td>644.87</td>\n",
       "      <td>516.95</td>\n",
       "      <td>465.13</td>\n",
       "      <td>558.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36169</th>\n",
       "      <td>667.91</td>\n",
       "      <td>763.85</td>\n",
       "      <td>537.93</td>\n",
       "      <td>668.26</td>\n",
       "      <td>490.79</td>\n",
       "      <td>621.11</td>\n",
       "      <td>548.31</td>\n",
       "      <td>672.15</td>\n",
       "      <td>632.54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25981 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       physics  biology  history  English  geography  literature  Portuguese  \\\n",
       "28005   405.95   441.07   543.93   642.68     450.98      525.70      561.74   \n",
       "17302   509.22   514.19   476.64   518.26     470.93      560.50      449.40   \n",
       "27044   505.69   586.47   676.95   740.67     557.43      573.57      684.98   \n",
       "31774   432.28   621.59   515.54   469.30     537.18      549.01      426.03   \n",
       "1483    670.08   515.43   598.85   686.78     636.61      652.86      508.80   \n",
       "...        ...      ...      ...      ...        ...         ...         ...   \n",
       "18630   447.26   419.96   514.69   360.20     576.42      435.27      458.60   \n",
       "23873   679.87   713.81   679.53   708.97     678.52      739.79      724.57   \n",
       "10573   584.54   677.47   613.97   543.48     560.80      579.90      510.76   \n",
       "10922   607.08   611.81   491.59   577.75     557.43      644.87      516.95   \n",
       "36169   667.91   763.85   537.93   668.26     490.79      621.11      548.31   \n",
       "\n",
       "         math  chemistry  \n",
       "28005  429.42     447.41  \n",
       "17302  393.80     410.14  \n",
       "27044  626.14     558.67  \n",
       "31774  520.14     566.83  \n",
       "1483   635.35     819.01  \n",
       "...       ...        ...  \n",
       "18630  422.37     437.57  \n",
       "23873  653.90     767.87  \n",
       "10573  489.41     484.16  \n",
       "10922  465.13     558.67  \n",
       "36169  672.15     632.54  \n",
       "\n",
       "[25981 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Error: 0.005091800554208521\n"
     ]
    }
   ],
   "source": [
    "# We already loaded X and y, but do it again as a reminder\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "# Split the data into training and testing sets (60% train, 40% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, shuffle=True)\n",
    "\n",
    "# Display the training data.\n",
    "display(X_train)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing data\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Compute the average error on the testing data\n",
    "average_error = (predictions - y_test).mean()\n",
    "\n",
    "print(\"Average Error:\", average_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the training data is a new DataFrame that maintains the indices from the original DataFrame. This makes it easier to look up corresponding values (e.g., in the label Series). Try setting `shuffle=false` in the previous python cell, and notice how it changes the indices. Turn shuffling back on (and re-run the cell again) before continuing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "Wow, these predictions are *really* good! Given the 9 entrance exam scores we can predict a new applicants GPA to within a couple *thousandths* of a GPA point!\n",
    "\n",
    "Lets look at some of these super-accurate predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         label  prediction  difference\n",
      "0          NaN     3.01000         NaN\n",
      "1          NaN     3.64333         NaN\n",
      "2          NaN     3.86000         NaN\n",
      "3          NaN     2.83333         NaN\n",
      "4          NaN     2.51667         NaN\n",
      "...        ...         ...         ...\n",
      "43289  3.75667         NaN         NaN\n",
      "43291  3.08667         NaN         NaN\n",
      "43297  3.63333         NaN         NaN\n",
      "43298  2.76333         NaN         NaN\n",
      "43300  3.75000         NaN         NaN\n",
      "\n",
      "[27734 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# The predictions are a numpy array. Convert them to a Series\n",
    "predictions_series = pd.Series(predictions, name='prediction')\n",
    "\n",
    "# Calculate the difference\n",
    "difference = predictions_series - y_test\n",
    "\n",
    "# Create a new DataFrame\n",
    "temp = pd.DataFrame({\n",
    "    'label': y_test,\n",
    "    'prediction': predictions_series,\n",
    "    'difference': difference\n",
    "})\n",
    "\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait, why are we getting NaN? Notice that this DataFrame has 43,300 rows, which is roughly the total number of data points. This should only be the length of the testing set, which is far smaller!\n",
    "\n",
    "What's happening is that `train_test_split` preserves the original indexing when producing `y_test`. So, although `y_test.size == 17322`, the indexes of `y_test` span from 0 to 43,302. This is useful in cases where you want to match up labels in `y_test` to their corresponding rows in the original data set. \n",
    "\n",
    "We can use `reset_index(drop=True)` [[link]](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reset_index.html) to reset the indices in `y_test` to the default indexing of 0, 1, 2, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         label  prediction  difference\n",
      "0      2.38000    3.010000    0.630000\n",
      "1      2.94333    3.643330    0.700000\n",
      "2      3.58333    3.860000    0.276670\n",
      "3      3.79000    2.833330   -0.956670\n",
      "4      2.44333    2.516670    0.073340\n",
      "...        ...         ...         ...\n",
      "17317  2.87000    3.500000    0.630000\n",
      "17318  3.92667    3.593330   -0.333340\n",
      "17319  2.03667    0.473333   -1.563337\n",
      "17320  3.62333    0.266667   -3.356663\n",
      "17321  3.35667    2.583330   -0.773340\n",
      "\n",
      "[17322 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# The predictions are a numpy array. Convert them to a Series\n",
    "predictions_series = pd.Series(predictions, name='prediction')\n",
    "y_test_series = pd.Series(y_test, name='label').reset_index(drop=True) # We reset the indices in y_test. drop=True means to discard the old indices. If False, it keeps the old index as a new column rather than discarding it.\n",
    "\n",
    "# Calculate the difference\n",
    "difference = predictions_series - y_test_series\n",
    "\n",
    "# Create a new DataFrame\n",
    "temp = pd.DataFrame({\n",
    "    'label': y_test_series,\n",
    "    'prediction': predictions_series,\n",
    "    'difference': difference\n",
    "})\n",
    "\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something's wrong here! These aren't that accurate. Almost all are off by way more than a few thousandths of a GPA point. \n",
    "\n",
    "Before going on, note that we could obtain these values (in a different order) with the following. Here `predictions` is a numpy array, while `y_test` is a Series, so the result is a Series. The Series includes index information. Notice that the indices are not in order. The previous discussion should make it clear why this is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23161    0.630000\n",
      "2278     0.700000\n",
      "24792    0.276670\n",
      "30070   -0.956670\n",
      "35701    0.073340\n",
      "           ...   \n",
      "19475    0.630000\n",
      "10036   -0.333340\n",
      "7515    -1.563337\n",
      "28792   -3.356663\n",
      "12557   -0.773340\n",
      "Name: gpa, Length: 17322, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(predictions - y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: These errors seem bigger than expected based on our evaluation. What are we doing wrong?\n",
    "\n",
    "**Answer**: We are computing the mean error (or average error), which lets positive and negative errors cancel out! This measures whether we are on average over-predicting or under-predicting. We are (on average) under-predicting by a slight amount.\n",
    "\n",
    "There are several alternative metrics that can better quantify the accuracy of a model for a regression problem. We review four of the most common:\n",
    "\n",
    "#### Mean Squared Error (MSE)\n",
    "\n",
    "MSE measures the average of the squares of the errors. It gives a higher weight to larger errors, making it sensitive to outliers. It's useful when large errors are particularly undesirable.\n",
    "\n",
    "$$\\operatorname{MSE}=\\frac{1}{n}\\sum_{i=1}^n (y_i-\\hat y_i)^2,$$\n",
    "\n",
    "where $n$ is the size of the testing set, $y_i$ is the $i^\\text{th}$ label, and $\\hat y_i$ is the $i^\\text{th}$ prediction.\n",
    "\n",
    "#### Root Mean Squared Error (RMSE)\n",
    "\n",
    "RMSE is the square root of MSE. It has the same units as the target variable (the same scale), making  it easier to interpret. Like MSE, it gives more weight to larger errors.\n",
    "\n",
    "$$\\operatorname{RMSE}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n (y_i-\\hat y_i)^2}.$$\n",
    "\n",
    "#### Mean Absolute Error (MAE)\n",
    "MAE measures the average magnitude of the errors in a set of predictions, without considering their sign. It's less sensitive to outliers compared to MSE and RMSE (this can be a good thing or a bad thing, depending on your application).\n",
    "\n",
    "$$\\operatorname{MAE}=\\frac{1}{n}\\sum_{i=1}^n \\left \\vert y_i - \\hat y_i \\right \\vert.$$\n",
    "\n",
    "#### R-squared ($R^2$)\n",
    "\n",
    "R-squared, or the *coefficient of determination*, indicates the proportion of the variance of the dependent variable (labels) that is predictable from the independent variables (predictions). Unlike the other metrics, a higher $R^2$ indicates a better fit.\n",
    "\n",
    "$$R^2=1-\\frac{\\sum_{i=1}^n (y_i-\\hat y_i)^2}{\\sum_{i=1}^n (y_i - \\bar y)^2},$$\n",
    "\n",
    "where $\\bar y = \\frac{1}{n}\\sum_{i=1}^n y_i$ is the average label. In this equation the numerator measures the unexplained variance by the model and the denominator measures the total variance in the actual labels.\n",
    "\n",
    "\n",
    "Let's create functions for computing these different metrics given an array or Series of predictions and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(predictions, labels):\n",
    "    return np.mean((predictions - labels) ** 2)\n",
    "\n",
    "def root_mean_squared_error(predictions, labels):\n",
    "    return np.sqrt(mean_squared_error(predictions, labels))\n",
    "\n",
    "def mean_absolute_error(predictions, labels):\n",
    "    return np.mean(np.abs(predictions - labels))\n",
    "\n",
    "def r_squared(predictions, labels):\n",
    "    ss_res = np.sum((labels - predictions) ** 2)        # ss_res is the \"Sum of Squares of Residuals\"\n",
    "    ss_tot = np.sum((labels - np.mean(labels)) ** 2)    # ss_tot is the \"Total Sum of Squares\"\n",
    "    return 1 - (ss_res / ss_tot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use these functions to test how well our NN algorithm works on the GPA data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Error: 0.005091800554208521\n",
      "Mean Squared Error: 1.1341230826549182\n",
      "Root Mean Squared Error: 1.0649521504062605\n",
      "Mean Absolute Error: 0.8192533543932571\n",
      "R-squared: -0.6876057456137379\n"
     ]
    }
   ],
   "source": [
    "# Compute the average error and other metrics on the testing data\n",
    "average_error = (predictions - y_test).mean()\n",
    "mse = mean_squared_error(predictions, y_test)\n",
    "rmse = root_mean_squared_error(predictions, y_test)\n",
    "mae = mean_absolute_error(predictions, y_test)\n",
    "r2 = r_squared(predictions, y_test)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"Average Error:\", average_error)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These give a much clearer picture of how accurate the model is. Some area easier to interpret than others, but all can be used to compare the performance of different ML methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}