{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPSCI 389: Homework 2 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Short Answer\n",
    "\n",
    "Answer the following questions with at least a few sentences, and no more than roughly one page of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. [10 points] What is the the difference between a parametric and a non-parametric method in machine learning?\n",
    "\n",
    "***Answer***\n",
    "\n",
    "<font color=\"white\">\n",
    "Parametric methods, like linear regression, have a fixed number of parameters that doesn't change with the size of the training data. These models also make assumptions about the form (or shape) of the function being approximated. For example, when using the polynomial basis, linear parametric models represent the assumption that the relationship between the inputs and outputs can be represented by a polynomial.\n",
    "\n",
    "Non-parametric methods, including nearest neighbor, k-nearest neighbor, and weighted k-nearest neighbor, don't have a fixed number of parameters. They do not make explicit assumptions about the form of the function relating inputs to labels.\n",
    "</font>\n",
    "\n",
    "---\n",
    "\n",
    "***Updated Answer***\n",
    "\n",
    "<font color=\"Green\">\n",
    "Your answer should recognize that:\n",
    "\n",
    "1. Parametric models have a fixed number of parameters.\n",
    "2. Non-parametric models do not have a fixed number of parameters.\n",
    "3. Parametric models make an assumption about the form (shape) of the function that relates inputs to outputs.\n",
    "4. Non-parametric models do not make an assumption about the form of the function that relates inputs to outputs.\n",
    "\n",
    "You answer should include (1 or 2) and (3 or 4). It need not include all four points, since some are implied by stating others, e.g., an answer stating 1 can be viewed as implicitly stating 2.\n",
    "\n",
    "**Reflection**: If your answer was incomplete, in the updated answer include the components that you missed.\n",
    "\n",
    "</font>\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. [20 points] State whether the following parametric model is a linear parametric model. Explain your answer.\n",
    "\n",
    "Let $g_v$ be a linear parametric model with weights $v \\in \\mathbb R^m$ and basis $\\phi$. We will create a new parametric model, $f_w$, with weights $w \\in \\mathbb R^{2m}$. Let $w=[v,v']$, i.e., let $w$ be the concatenation of two different weight vectors $v \\in \\mathbb R^m$ and $v' \\in \\mathbb R^m$. Then, let $f_w$ be defined as:\n",
    "$$\n",
    "f_w(x_i) = \\max\\Big \\{ g_{v}(x_i), g_{v'}(x_i)\\Big \\}.\n",
    "$$\n",
    "\n",
    "Is this parametric model, $f_w$, a linear parametric model? Why or why not?\n",
    "\n",
    "**Note**: You may use any online resources (e.g., ChatGPT) to help you format equations using LaTeX.\n",
    "\n",
    "***Answer***\n",
    "\n",
    "<font color=\"white\">\n",
    "\n",
    "No, this is not a linear parametric model. We can verify this by showing that the derivative with respect to the weights, $w$, is an expression that depends on (varies with) $w$.\n",
    "\n",
    "Consider a weight, $w_k$, that corresponds to a weight in $v$ (not $v'$). Specifically, let it be $v_k$, the $k^\\text{th}$ weight in $v$. We then have:\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\frac{\\partial f_w(x_i)}{\\partial v_k} =& \\frac{\\partial}{\\partial v_k} \\max\\Big \\{ g_{v}(x_i), g_{v'}(x_i)\\Big \\}\\\\\n",
    "    =&\n",
    "        \\begin{cases}\n",
    "            \\frac{\\partial}{\\partial v_k}g_{v}(x_i) &\\text{ if }  g_{v}(x_i) >  g_{v'}(x_i)\\\\\n",
    "            \\frac{\\partial}{\\partial v_k}g_{v'}(x_i) &\\text{ if }  g_{v}(x_i) <  g_{v'}(x_i)\\\\\n",
    "            \\text{undefined} &\\text{ if }  g_{v}(x_i) =  g_{v'}(x_i)\n",
    "        \\end{cases}\\\\\n",
    "    =&\n",
    "        \\begin{cases}\n",
    "            \\frac{\\partial}{\\partial v_k}\\sum_{j=1}^m v_j \\phi_j(x_i) &\\text{ if }  g_{v}(x_i) >  g_{v'}(x_i)\\\\\n",
    "            0 &\\text{ if }  g_{v}(x_i) <  g_{v'}(x_i)\\\\\n",
    "            \\text{undefined} &\\text{ if }  g_{v}(x_i) =  g_{v'}(x_i)\n",
    "        \\end{cases}\\\\\n",
    "    =&\n",
    "        \\begin{cases}\n",
    "            \\phi_k(x_i) &\\text{ if }  g_{v}(x_i) >  g_{v'}(x_i)\\\\\n",
    "            0 &\\text{ if }  g_{v}(x_i) <  g_{v'}(x_i)\\\\\n",
    "            \\text{undefined} &\\text{ if }  g_{v}(x_i) =  g_{v'}(x_i).\n",
    "        \\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "If $\\phi_k(x_i) \\neq 0$ for some $x_i$, this derivative varies with $v_k$. This is because there are values of $v_k$ for which $g_{v}(x_i) >  g_{v'}(x_i)$ (and so the derivative is $\\phi_k(x_i)$), and there are other values of $v_k$ for which $g_{v}(x_i) <  g_{v'}(x_i)$ (and so the derivative is $0$). Hence, for different values of $v_k$ the value of the derivative can change.\n",
    "\n",
    "</font>\n",
    "\n",
    "---\n",
    "\n",
    "***Updated Answer***\n",
    "\n",
    "<font color=\"Green\">\n",
    "\n",
    "**Reflection**: If you indicated that the model is a linear parametric model, review the proof above that it is not a linear parametric model. Comment on where your answer went wrong and summarize why the model is not a linear parametric model. This summary may be in English (you do not need to copy this proof). If you indicated that the model is not a linear parametric model, comment on whether you provided a complete argument for \"why or why not\". If you did not provide any mathematical proof (that is, if you did not work out the derivative), in your reflection include the derivatives. Notice that you do not need to work out the entire derivation above. Rather, you need only show that for two different values of $w$ the derivative can take different values. If your answer was correct and included a complete derivation showing that the derivative does not depend on $w$, your answer may simply indicate \"My answer correctly stated the model is not linear, and included the necessary derivations to show this.\"\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. [10 points] How can you estimate the the mean squared error (MSE) of a particular parametric ML model $f_w$ given access to data, $D=(X_i,Y_i)_{i=1}^n$?\n",
    "\n",
    "**Note**: Here the weights $w$ have already been determined independent of the data $D$. You may write $f_w(x)$ to denote the prediction made by the model when given input $x$.\n",
    "\n",
    "***Answer***\n",
    "\n",
    "<font color=\"white\">\n",
    "\n",
    "You can compute the sample mean squared error on the data:\n",
    "\n",
    "$$\n",
    "\\operatorname{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - f_w(x_i))^2.\n",
    "$$\n",
    "\n",
    "</font>\n",
    "\n",
    "---\n",
    "\n",
    "***Updated Answer***\n",
    "\n",
    "<font color=\"Green\">\n",
    "\n",
    "**Reflection**: Notice that the data does not need to be split into training and testing data, since the model was already trained on different data. Hence, $D$ corresponds to the testing data. If you indicated that data should be split into training and testing, make a note of this. If your answer suggested using k-fold cross-validation, read the solution to the next problem and then comment here on why k-fold cross-validation is not the appropriate tool to use here. If your answer was correct, you do not need to provide additional reflection for this question.\n",
    "\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. [10 points] How can you estimate the mean squared error (MSE) that would result if you used a specific ML algorithm `alg` to train a model using $m$ data points? \n",
    "\n",
    "You may assume access to $n>m$ data points. Your answer may be short - specifying a method or approach that we have discussed and why it is appropriate.\n",
    "\n",
    "***Initial Answer***\n",
    "\n",
    "<font color=\"white\">\n",
    "\n",
    "Use $k$-fold cross-validation. Recall that we use train/test splits to evaluate the performance/accuracy of a single model. However, a single train/test split only evaluates the performance/accuracy of the one model trained on the training data. Different training sets could result in different models, and so this doesn't capture how sensitive the algorithm for training the model is to variations in the training data (e.g., how sensitive the algorithm is to outliers in the training data). By training many different models using different subsets of the data, $k$-fold cross-validation estimates the performance/accuracy of a model trained using a specific algorithm on a data set of a specific size (as opposed to estimating the performance/accuracy of one single model).\n",
    "\n",
    "</font>\n",
    "\n",
    "---\n",
    "\n",
    "***Updated Answer***\n",
    "\n",
    "<font color=\"Green\">\n",
    "\n",
    "Your answer need only specify that $k$-fold cross validation should be used. It does not need to explain why. \n",
    "\n",
    "**Reflection**: If you recommended a different method, explain your reasoning and whether you still believe that your answer is correct (there may be other correct answers using techniques not discussed in class). If your answer was correct, you do not need to provide additional reflection for this question.\n",
    "\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. [10 points] Imagine that you have a current model for a regression problem with mean squared error (MSE) $2.0$. We have $n=150$ data points, we train a model using $100$. We evaluate the model by computing the sample mean squared error (MSE) on the other $50$ points and obtain a sample MSE of $1.2$. We compute the sample standard deviation of the squared errors on the $50$ testing points and obtain a standard deviation of $10$, can we conclude with reasonable confidence that the new model is better (has lower MSE) than the current model? Why or why not?\n",
    "\n",
    "***Answer***\n",
    "\n",
    "<font color=\"white\">\n",
    "\n",
    "The typical answer should be \"no\". The method for computing confidence intervals that we discussed in class was using the sample standard error times $1.96$. The sample standard error is the sample standard deviation divided by the square root of the number of samples. In this case, that is $10/\\sqrt{50}\\approx 1.412$. Multiplying this by $1.96$ we obtain a confidence interval width of approximately $2.77$. This is significantly larger than the difference between the current model's MSE of $2.0$ and the new model's sample MSE of $1.2$. So, we cannot conclude with $95\\%$ confidence that the new model has lower sample MSE (even under the assumptions necessary for this confidence interval to be valid). Most other confidence intervals are more conservative (wider), and will result in this same conclusion. \n",
    "\n",
    "It is possible that an acceptable answer would argue for \"yes\" using other techniques for establishing a confidence interval (likely using information that was not provided). For example, one could argue that \"Yes, we *could* conclude this if we had additional information and applied certain other confidence intervals.\" Such an answer should provide further detail about this other confidence interval and the necessary other information.\n",
    "\n",
    "</font>\n",
    "\n",
    "---\n",
    "\n",
    "***Updated Answer***\n",
    "\n",
    "<font color=\"Green\">\n",
    "\n",
    "**Reflection**: Comment on whether or not you believe your initial answer was correct. If you believe your answer is correct, but the math/justification differs from the one provided here, describe which argument you find more compelling, and why. If you believe your answer was not correct, comment on where it went wrong. If your believe your answer is correct and uses the math/justification above, comment on what the line \"even under the assumptions necessary for this confidence interval to be valid\" means and why this may often mean that we do *not* have \"reasonable confidence\".\n",
    "\n",
    "**Note that you should be entering a reflection for your updated answer regardless of whether or not your initial answer was correct.**\n",
    "\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. [20 points] Derive the gradient descent update equations for the following loss function.\n",
    "\n",
    "Let $L(w,D)=\\sum_{i=1}^n 1 + \\sin(2 \\pi (y_i - \\hat y_i - 1/4))$, where $y_i$ is the label associated with the $i^\\text{th}$ point and $\\hat y_i$ is the model's prediction of $y_i$. The code snippet below this question creates a plot of this loss function. Notice that this loss function indicates that the lass associated with an error scales with its distance from the closest integer. Errors equal to integer values result in no loss at all, while errors half way between integers result in the largest loss.\n",
    "\n",
    "Consider a linear parametric model:\n",
    "$$\n",
    "f_w(x_i) = \\frac{1}{n}\\sum_{j=1}^m w_j \\phi_j(x_i),\n",
    "$$\n",
    "where $\\phi(x_i) \\in \\mathbb R^m$. Derive the gradient update rule for $w_j$. Your final answer should not include any derivative or gradient symbols (work out what these derivatives/gradients are).\n",
    "\n",
    "**Note:** Remember that $\\frac{\\partial}{\\partial x} \\sin(f(x)) = \\cos(f(x)) \\frac{\\partial}{\\partial x} f(x)$.\n",
    "\n",
    "**Note:** This problem is worth 2-4 times the number of points of previous problems, and should take a correspondingly longer amount time to complete. I recommend working this out first with pencil and paper, double checking your derivatives, and then typing your answer.\n",
    "\n",
    "***Answer***\n",
    "\n",
    "<font color=\"white\">\n",
    "\n",
    "The gradient descent update for weight vector $w$ performs the following update for each weight $w_j$:\n",
    "\n",
    "$$\n",
    "w_j \\gets w_j - \\alpha \\frac{\\partial L(w,D)}{\\partial w_j}.\n",
    "$$\n",
    "\n",
    "Or, we might write $w_{i,j}$ to denote the weight $w_j$ during the $i^\\text{th}$ iteration. In this case, the update is:\n",
    "\n",
    "$$\n",
    "w_{i+1,j} = w_{i,j} - \\alpha \\frac{\\partial L(w_i,D)}{\\partial w_{i,j}}.\n",
    "$$\n",
    "\n",
    "We will use the former, since it avoid extra subscripts. In order to complete this update, we need to find an expression for $\\frac{\\partial L(w,D)}{\\partial w_j}$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\frac{\\partial L(w,D)}{\\partial w_j}=& \\frac{\\partial}{\\partial w_j} \\sum_{i=1}^n 1 + \\sin(2 \\pi (y_i - \\hat y_i - 1/4))\\\\\n",
    "    =& \\frac{\\partial}{\\partial w_j} \\sum_{i=1}^n 1 + \\sin \\left (2 \\pi \\left (y_i - f_w(x_i) - 1/4\\right ) \\right ).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Here we have replace the prediction $\\hat y_i$ in the loss function definition with the prediction produced by the parametric model, $f_w$. Next, we note that the constant $1$ does not depend on $w_j$, and so its derivative is zero. We also push the derivative through the $\\sin$ term, applying the rule provided in the question \"note\":\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial L(w,D)}{\\partial w_j} =  \\sum_{i=1}^n \\cos \\left (2 \\pi \\left (y_i - f_w(x_i) - 1/4\\right ) \\right ) \\frac{\\partial}{\\partial w_j}\\left (2 \\pi \\left (y_i - f_w(x_i) - 1/4\\right ) \\right ).\n",
    "$$\n",
    "\n",
    "Notice that $y_i$ and $-1/4$ do not depend on $w_j$, and so the latter terms simplify giving:\n",
    "$$\n",
    "    \\frac{\\partial L(w,D)}{\\partial w_j} =  \\sum_{i=1}^n \\cos \\left (2 \\pi \\left (y_i - f_w(x_i) - 1/4\\right ) \\right ) \\frac{\\partial}{\\partial w_j}\\left (2 \\pi \\left ( - f_w(x_i) \\right ) \\right ).\n",
    "$$\n",
    "Moving the $-2\\pi$ from before $f_w(x_i)$ to the start of the expression, we obtain:\n",
    "$$\n",
    "    \\frac{\\partial L(w,D)}{\\partial w_j} =  -2\\pi\\sum_{i=1}^n \\cos \\left (2 \\pi \\left (y_i - f_w(x_i) - 1/4\\right ) \\right ) \\frac{\\partial}{\\partial w_j}\\left ( f_w(x_i) \\right ) .\n",
    "$$\n",
    "Next, we substitute in the definition of $f_w(x_i)$ as a linear parametric model. That is, $f_w(x_i)=\\frac{1}{n}\\sum_{k=1}^m w_k \\phi_k(x_i)$. Notice that we use $k$ as the variable in the summation. This is because we have already used the variable $j$ (to indicate the weight that we are taking the derivative with respect to). We therefore obtain:\n",
    "$$\n",
    "    \\frac{\\partial L(w,D)}{\\partial w_j} =  -2\\pi\\sum_{i=1}^n \\cos \\left (2 \\pi \\left (y_i - f_w(x_i) - 1/4\\right ) \\right ) \\frac{\\partial}{\\partial w_j}\\frac{1}{n}\\sum_{k=1}^m w_k \\phi_k(x_i). \n",
    "$$\n",
    "Notice that only the $k=j$ term of the summation depends on $w_j$, and so the derivative of all of the other terms is zero. Hence, we obtain (also moving the $1/n$ to the start of the expression):\n",
    "$$\n",
    "    \\frac{\\partial L(w,D)}{\\partial w_j} =  \\frac{-2\\pi}{n}\\sum_{i=1}^n \\cos \\left (2 \\pi \\left (y_i - f_w(x_i) - 1/4\\right ) \\right ) \\frac{1}{n}\\frac{\\partial}{\\partial w_j} w_j \\phi_j(x_i).\n",
    "$$\n",
    "Solving this last derivative, we obtain:\n",
    "$$\n",
    "    \\frac{\\partial L(w,D)}{\\partial w_j} =  \\frac{-2\\pi}{n}\\sum_{i=1}^n \\cos \\left (2 \\pi \\left (y_i - f_w(x_i) - 1/4\\right ) \\right ) \\phi_j(x_i).\n",
    "$$\n",
    "Substituting this into the gradient descent equation we obtain the final answer:\n",
    "$$\n",
    "w_j \\gets w_j + \\alpha \\frac{2\\pi}{n}\\sum_{i=1}^n \\cos \\left (2 \\pi \\left (y_i - f_w(x_i) - 1/4\\right ) \\right ) \\phi_j(x_i).\n",
    "$$\n",
    "</font>\n",
    "\n",
    "---\n",
    "\n",
    "***Updated Answer***\n",
    "\n",
    "<font color=\"Green\">\n",
    "\n",
    "Check your derivations and ensure that your final expression is equivalent to the one presented in the solutions. If you spot an error in your derivations, correct this error and provide an udpated derivation (that reaches the same answer as the solutions) in your reflection.\n",
    "\n",
    "Notice the use of a different subscript, $k$, when inserting the definition of a linear parametric model. A common mistake is to re-use the symbol $j$ here, which can result in the incorrect answer.\n",
    "\n",
    "**Regardless of whether your initial answer was correct,** reflect on this process, noting roughly how long this derivation took you. Did you make mistakes that you caught upon double-checking your answer? Does this approach of manually computing derivatives for gradient descent seem like a good strategy for training future parametric ML models?\n",
    "\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Programming\n",
    "\n",
    "The code below defines `evaluate_features`, which performs $k$-fold cross-validation to estimate the MSE that results when a linear parametric model is trained on the provided data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Function to perform k-fold cross-validation for regression\n",
    "def evaluate_features(X, y, n_splits=50, random_state=42):\n",
    "    \"\"\"\n",
    "    Evaluate features using k-fold cross-validation for regression.\n",
    "\n",
    "    Parameters:\n",
    "    X (DataFrame): The feature set.\n",
    "    y (Series): The labels (continuous values).\n",
    "    n_splits (int): The number of folds for cross-validation.\n",
    "    random_state (int): The seed used by the random number generator.\n",
    "\n",
    "    Returns:\n",
    "    float: The average MSE across all folds.\n",
    "    \"\"\"\n",
    "    # Create a pipeline with a standard scaler and a linear regression model\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', LinearRegression())\n",
    "    ])\n",
    "\n",
    "    # Define the k-fold cross-validation method\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # Perform cross-validation and calculate MSE\n",
    "    scores_mse = cross_val_score(pipeline, X, y, cv=kf, scoring=make_scorer(mean_squared_error))\n",
    "\n",
    "    return scores_mse.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below loads the GPA data and uses `evaluate_features` to determine the MSE likely to result from trainin a linear model on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MSE: 0.5820102673423404\n"
     ]
    }
   ],
   "source": [
    "# Load the data set\n",
    "df = pd.read_csv(\"https://people.cs.umass.edu/~pthomas/courses/COMPSCI_389_Spring2024/GPA.csv\", delimiter=',') # Read GPA.csv, assuming numbers are separated by commas\n",
    "\n",
    "# Split the data into features and labels\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "# Evaluate the features\n",
    "average_mse = evaluate_features(X, y)\n",
    "print(f\"Average MSE: {average_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the use of a fixed random number seed, each time you run this you should obtain the same result:\n",
    "> Average MSE: 0.5820105070662254\n",
    "\n",
    "#### 1. [20 points] In the code block below, perform **feature engineering** to come up with additional features that improve the predictions, reducing the reported average MSE to at most $0.573$.\n",
    "\n",
    "**Note**: There exist features that achieve an average MSE *less than* $0.57$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Answer***\n",
    "\n",
    "<font color=\"blue\">\n",
    "\n",
    "There are many possible answers. Below we provide one: adding each feature squared. This is sufficient to obtain the necessary average MSE. To obtain an average MSE less than $0.57$, uncomment the additional lines. These lines add the features cubed as well as the pairwise product of all exam scores.\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MSE: 0.5724093128820791\n"
     ]
    }
   ],
   "source": [
    "# NOTE: We recommend leaving these initial lines, so that if you re-run this cell many times you don't append multiple new features to the data set unintentionally\n",
    "df = pd.read_csv(\"https://people.cs.umass.edu/~pthomas/courses/COMPSCI_389_Spring2024/GPA.csv\", delimiter=',') # Read GPA.csv, assuming numbers are separated by commas\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "####################################################################################\n",
    "# ANSWER BELOW\n",
    "exam_scores = ['physics', 'biology', 'history', 'English', 'geography', 'literature', 'Portuguese', 'math', 'chemistry']\n",
    "\n",
    "for score in exam_scores:\n",
    "    X[f'{score}_squared'] = X[score] ** 2\n",
    "\n",
    "# UNCOMMENT THE LINES BELOW TO OBTAIN A LOWER AVERAGE MSE\n",
    "#for score in exam_scores:\n",
    "#    X[f'{score}_squared'] = X[score] ** 3\n",
    "\n",
    "#for i in range(len(exam_scores)):\n",
    "#    for j in range(i+1, len(exam_scores)):\n",
    "#        X[f'{exam_scores[i]}_{exam_scores[j]}_interaction'] = X[exam_scores[i]] * X[exam_scores[j]]\n",
    "####################################################################################\n",
    "\n",
    "# Evaluate the features\n",
    "average_mse = evaluate_features(X, y)\n",
    "print(f\"Average MSE: {average_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Updated Answer***\n",
    "\n",
    "<font color=\"Green\">\n",
    "\n",
    "**Reflection**: If you did not achieve an average MSE below $0.573$, comment on why this happened or what you struggled with. If you did achieve an average MSE below $0.573$, you do not need to provide additional reflection here.\n",
    "\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. [10 points] Reflect on this feature engineering process.\n",
    "\n",
    "Write a paragraph reflecting on the feature engineering process. What kinds of features did you try first? Did you change approaches or change the types of features you added? Did you try including features that were linear combinations of existing features (that is, weighted summations of existing features, like averages of a subset of the features)? Did you then make a deliberate effort to create new features that are not linear combinations of existing features? Why do you think the best features you found are effective?\n",
    "\n",
    "***Answer***\n",
    "\n",
    "<font color=\"white\">\n",
    "\n",
    "Answers may vary. In my case, I initially deliberately experimented with features that were linear combinations of others. The example feature provided (to show how features can be added) is one such feature, and I was surprised to find that it did result in a different MSE. I believe that this is due to the least squares solver within the linear regression model finding an *approximation* of the best weights, and this approximation differs when these features are added. In theory, additional features that are linear combinations of others should not influence the resulting model. I tried experimenting with features related to the variance of STEM and non-STEM exam scores, but found that these were not effective. In the end, I took inspiration from the polynomial basis to add in the square of each existing feature. I also recognized that the relationships between different exam scores could be interesting and relevant, and so I included the products of each pair of exams scores. I think that these features are effective because they capture common non-linearities (curvature) through the squared and cubic terms, and the relationships between exam scores through the pairwise product terms.\n",
    "\n",
    "</font>\n",
    "\n",
    "---\n",
    "\n",
    "***Updated Answer***\n",
    "\n",
    "<font color=\"Green\">\n",
    "\n",
    "Ensure that your answer is roughly at the level of detail of the answer provided above - it should be at least one full paragraph.\n",
    "\n",
    "</font>\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
